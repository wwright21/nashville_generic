{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import warnings\n",
    "import topojson as tp\n",
    "import requests\n",
    "import os\n",
    "from parcllabs import ParclLabsClient\n",
    "from dotenv import load_dotenv\n",
    "from requests.exceptions import RequestException\n",
    "import datetime\n",
    "\n",
    "# set metro county dictionary\n",
    "county_dict = {\n",
    "    '47015': 'Cannon',\n",
    "    '47021': 'Cheatham',\n",
    "    '47037': 'Davidson',\n",
    "    '47043': 'Dickson',\n",
    "    '47081': 'Hickman',\n",
    "    '47111': 'Macon',\n",
    "    '47119': 'Maury',\n",
    "    '47147': 'Robertson',\n",
    "    '47149': 'Rutherford',\n",
    "    '47159': 'Smith',\n",
    "    '47165': 'Sumner',\n",
    "    '47169': 'Trousdale',\n",
    "    '47187': 'Williamson',\n",
    "    '47189': 'Wilson'\n",
    "}\n",
    "\n",
    "# nationwide FIPS codes\n",
    "fips_dict = {\n",
    "    '01': 'AL',\n",
    "    '02': 'AK',\n",
    "    '04': 'AZ',\n",
    "    '05': 'AR',\n",
    "    '06': 'CA',\n",
    "    '08': 'CO',\n",
    "    '09': 'CT',\n",
    "    '10': 'DE',\n",
    "    '11': 'DC',\n",
    "    '12': 'FL',\n",
    "    '13': 'GA',\n",
    "    '15': 'HI',\n",
    "    '16': 'ID',\n",
    "    '17': 'IL',\n",
    "    '18': 'IN',\n",
    "    '19': 'IA',\n",
    "    '20': 'KS',\n",
    "    '21': 'KY',\n",
    "    '22': 'LA',\n",
    "    '23': 'ME',\n",
    "    '24': 'MD',\n",
    "    '25': 'MA',\n",
    "    '26': 'MI',\n",
    "    '27': 'MN',\n",
    "    '28': 'MS',\n",
    "    '29': 'MO',\n",
    "    '30': 'MT',\n",
    "    '31': 'NE',\n",
    "    '32': 'NV',\n",
    "    '33': 'NH',\n",
    "    '34': 'NJ',\n",
    "    '35': 'NM',\n",
    "    '36': 'NY',\n",
    "    '37': 'NC',\n",
    "    '38': 'ND',\n",
    "    '39': 'OH',\n",
    "    '40': 'OK',\n",
    "    '41': 'OR',\n",
    "    '42': 'PA',\n",
    "    '44': 'RI',\n",
    "    '45': 'SC',\n",
    "    '46': 'SD',\n",
    "    '47': 'TN',\n",
    "    '48': 'TX',\n",
    "    '49': 'UT',\n",
    "    '50': 'VT',\n",
    "    '51': 'VA',\n",
    "    '53': 'WA',\n",
    "    '54': 'WV',\n",
    "    '55': 'WI',\n",
    "    '56': 'WY'\n",
    "}\n",
    "\n",
    "url = 'https://transition.fcc.gov/oet/info/maps/census/fips/fips.txt'\n",
    "\n",
    "# Fetch the content from the URL\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Check that the request was successful\n",
    "\n",
    "table = response.text.split('------------    --------------\\n')[1]\n",
    "\n",
    "# Strip leading/trailing whitespace and split by newline\n",
    "lines = table.strip().split('\\n')\n",
    "\n",
    "# Create a DataFrame from the list of lines\n",
    "df = pd.DataFrame(lines, columns=['Data'])\n",
    "\n",
    "# Split the 'Data' column on the first space\n",
    "df[['FIPS', 'County_name']] = df['Data'].str.split(n=1, expand=True)\n",
    "\n",
    "# Drop the original 'Data' column\n",
    "df = df.drop(columns=['Data'])\n",
    "\n",
    "# Drop rows where 'FIPS' ends with '000'\n",
    "df = df[~df['FIPS'].str.endswith('000')]\n",
    "\n",
    "# Extract the first 2 digits from 'FIPS' column\n",
    "df['State_code'] = df['FIPS'].str[:2]\n",
    "\n",
    "# Map 'State_code' to 'State' using fips_dict\n",
    "df['State'] = df['State_code'].map(fips_dict)\n",
    "\n",
    "# Drop the 'State_code' column if not needed\n",
    "df = df.drop(columns=['State_code'])\n",
    "\n",
    "df['county_state'] = df['County_name'] + ', ' + df['State']\n",
    "\n",
    "# Create dictionary using zip and to_dict\n",
    "nationwide_FIPSdict = dict(zip(df['FIPS'], df['county_state']))\n",
    "\n",
    "\n",
    "def get_dates(months_ago):\n",
    "    \"\"\"Gets today's date and a date `months_ago` months ago.\n",
    "\n",
    "    Args:\n",
    "      months_ago: The number of months to go back.\n",
    "\n",
    "    Returns:\n",
    "      A tuple containing today's date and the date `months_ago` months ago.\n",
    "    \"\"\"\n",
    "\n",
    "    start_date = datetime.date.today()\n",
    "    start_date_formatted = start_date.strftime(\"%Y-%m-%d\")\n",
    "    # Assuming 30.44 days per month on average\n",
    "    end_date = (start_date - datetime.timedelta(days=months_ago *\n",
    "                30.44)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    return start_date_formatted, end_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simplify tracts, derive counties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore the warnings that come with simplifying geographically\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# simplify tracts --------------------------------------\n",
    "tracts = gpd.read_file('tract_outlines.gpkg')\n",
    "tracts['FIPS'] = tracts['STATEFP'] + tracts['COUNTYFP']\n",
    "tracts = tracts[[\n",
    "    'FIPS',\n",
    "    'GEOID',\n",
    "    'geometry'\n",
    "]]\n",
    "\n",
    "tracts['county_name'] = tracts['FIPS'].map(nationwide_FIPSdict)\n",
    "\n",
    "toposimplify_tracts = 0.001\n",
    "tracts_simp = tp.Topology(tracts, toposimplify=toposimplify_tracts).to_gdf()\n",
    "tracts_simp.to_file('tracts_simp.gpkg')\n",
    "\n",
    "# create the counties by dissolving the tracts on the FIPS column\n",
    "counties = tracts.dissolve(by='FIPS').reset_index()\n",
    "counties = counties.drop(columns='GEOID')\n",
    "counties['county_name'] = counties['FIPS'].map(nationwide_FIPSdict)\n",
    "counties['county_stripped'] = counties['county_name'].apply(\n",
    "    lambda x: x.split(' County,')[0])\n",
    "counties = counties[[\n",
    "    'FIPS',\n",
    "    'county_name',\n",
    "    'county_stripped',\n",
    "    'geometry'\n",
    "]]\n",
    "\n",
    "# export simplified geometry\n",
    "counties.to_file('counties_simp.gpkg')\n",
    "print('export complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert STDB Excel files to CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to open each Excel file downloaded from STDB, make a small change, and save\n",
    "# Then run this script\n",
    "def convert_excel_to_csv(directory, output_directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.startswith(\"Color-coded maps\") and filename.endswith(\".xlsx\"):\n",
    "            # Construct the full path to the Excel file\n",
    "            excel_path = os.path.join(directory, filename)\n",
    "\n",
    "            # Read the Excel file into a DataFrame\n",
    "            df = pd.read_excel(excel_path, engine='openpyxl')\n",
    "\n",
    "            # Ensure the \"Census Tract\" column is of type object (string)\n",
    "            df['Census Tract'] = df['Census Tract'].astype(str)\n",
    "\n",
    "            # Rename the \"Census Tract\" column to \"GEOID\"\n",
    "            df.rename(columns={'Census Tract': 'GEOID'}, inplace=True)\n",
    "\n",
    "            # Construct the full path for the output CSV file\n",
    "            csv_filename = filename.replace(\".xlsx\", \".csv\")\n",
    "            csv_path = os.path.join(output_directory, csv_filename)\n",
    "\n",
    "            # Save the DataFrame to a CSV file\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            print(f\"Converted {filename} to {csv_filename}\")\n",
    "\n",
    "\n",
    "convert_excel_to_csv('Data/', 'Data/CSV/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file('Data/counties_simp.gpkg')\n",
    "\n",
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a county total net dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countyTotal_inflow = pd.read_csv('Data/inflow_CountyTotal.csv')\n",
    "countyTotal_inflow['merge_ID'] = countyTotal_inflow['destination_FIPS'].astype(\n",
    "    str) + '-' + countyTotal_inflow['year'].astype(str)\n",
    "\n",
    "countyTotal_outflow = pd.read_csv('Data/outflow_CountyTotal.csv')\n",
    "countyTotal_outflow['merge_ID'] = countyTotal_outflow['origin_FIPS'].astype(\n",
    "    str) + '-' + countyTotal_outflow['year'].astype(str)\n",
    "\n",
    "df_merged = pd.merge(\n",
    "    countyTotal_inflow,\n",
    "    countyTotal_outflow,\n",
    "    on='merge_ID'\n",
    ")\n",
    "\n",
    "df_merged = df_merged.rename(columns={\n",
    "    'destination_FIPS': 'FIPS',\n",
    "    'year_x': 'year',\n",
    "    'destination_county': 'county_name'\n",
    "})\n",
    "\n",
    "df_merged = df_merged[[\n",
    "    'year',\n",
    "    'FIPS',\n",
    "    'county_name',\n",
    "    'people_inflow',\n",
    "    'agi_inflow',\n",
    "    'agi_capita_inflow',\n",
    "    'people_outflow',\n",
    "    'agi_outflow',\n",
    "    'agi_capita_outflow'\n",
    "]]\n",
    "\n",
    "df_merged = df_merged.sort_values(by='year')\n",
    "\n",
    "df_merged['people_net'] = df_merged['people_inflow'] - \\\n",
    "    df_merged['people_outflow']\n",
    "df_merged['agi_net'] = df_merged['agi_inflow'] - df_merged['agi_outflow']\n",
    "df_merged.to_csv('Data/netflow_CountyTotal.csv', index=False)\n",
    "\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Metro migration total \"series\" to run in parallel with selected county\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate migration for each year\n",
    "metro_data = df_merged.groupby('year').agg({\n",
    "    'people_net': 'sum',\n",
    "    'agi_net': 'sum',\n",
    "    'people_inflow': 'sum',\n",
    "    'agi_inflow': 'sum',\n",
    "    'people_outflow': 'sum',\n",
    "    'agi_outflow': 'sum'\n",
    "}).reset_index()\n",
    "metro_data['FIPS'] = 'n/a'\n",
    "metro_data['county_name'] = 'Metro'\n",
    "metro_data['agi_capita_inflow'] = 0\n",
    "metro_data['agi_capita_outflow'] = 0\n",
    "\n",
    "metro_data = metro_data[[\n",
    "    'year',\n",
    "    'FIPS',\n",
    "    'county_name',\n",
    "    'people_inflow',\n",
    "    'agi_inflow',\n",
    "    'agi_capita_inflow',\n",
    "    'people_outflow',\n",
    "    'agi_outflow',\n",
    "    'agi_capita_outflow',\n",
    "    'people_net',\n",
    "    'agi_net'\n",
    "]]\n",
    "\n",
    "# Concatenate the metrowide with the original, filtered data\n",
    "df_final = pd.concat([df_merged, metro_data], ignore_index=True)\n",
    "\n",
    "df_final['county_name'] = df_final['county_name'].str.split(\n",
    "    ' County', expand=True)[0]\n",
    "\n",
    "\n",
    "df_final.to_csv('Data/netflow_MetroTotal.csv', index=False)\n",
    "df_final.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get county-level demographic KPIs\n",
    "import glob\n",
    "csv_files = glob.glob('County_CSV/*.csv')\n",
    "\n",
    "df_master = pd.read_csv(csv_files[0])\n",
    "\n",
    "# for file in csv_files[1:]:\n",
    "#     df_temp = pd.read_csv(file)\n",
    "#     df_master = df_master.merge(\n",
    "#         df_temp,\n",
    "#         on='County',\n",
    "#         how='left',\n",
    "#         suffixes=('', '_' + file.split('/')[-1].split('.')[0])\n",
    "#     )\n",
    "\n",
    "# df_master['county_name'] = df_master['County'].apply(\n",
    "#     lambda x: x.split(' County')[0])\n",
    "\n",
    "# df_master = df_master[df_master['county_name'].isin(county_dict.values())]\n",
    "\n",
    "# df_master['2024 Population 55+'] = df_master['2024 Population Age 55-59'].str.replace(',', '').astype(int) + \\\n",
    "#     df_master['2024 Population Age 60-64'].str.replace(',', '').astype(int) + \\\n",
    "#     df_master['2024 Senior Population'].str.replace(',', '').astype(int)\n",
    "# df_master['2024 Population 55+'] = df_master['2024 Population 55+'].apply(\n",
    "#     lambda x: f'{x:,.0f}')\n",
    "\n",
    "# df_master = df_master.drop(columns=[\n",
    "#     'County',\n",
    "#     '2024 Population Age 60-64',\n",
    "#     '2024 Population Age 55-59',\n",
    "#     '2024 Senior Population'\n",
    "# ])\n",
    "\n",
    "# cols = list(df_master)\n",
    "# cols.insert(0, cols.pop(cols.index('county_name')))\n",
    "# df_master = df_master.loc[:, cols]\n",
    "# df_master.to_csv('County_CSV/countyKPI.csv', index=False)\n",
    "# df_master.head(4)\n",
    "df_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get census tract showing 55+ age bracket\n",
    "df_senior = pd.read_csv(\n",
    "    'CSV/Color-coded maps - 2024 Senior Population.csv', dtype={'GEOID': 'str'})\n",
    "# df_senior['GEOID'] = df_senior['GEOID'].astype(float).map(\n",
    "#     lambda x: '{:.2f}'.format(x).replace('.', ''))\n",
    "\n",
    "df_55 = pd.read_csv(\n",
    "    'CSV/Color-coded maps - 2024 Population Age 55-59.csv', dtype={'Census Tract': 'str'})\n",
    "df_55 = df_55.rename(columns={\n",
    "    'Census Tract': 'GEOID'\n",
    "})\n",
    "# df_55['GEOID'] = df_55['GEOID'].astype(float).map(\n",
    "#     lambda x: '{:.2f}'.format(x).replace('.', ''))\n",
    "\n",
    "df_60 = pd.read_csv(\n",
    "    'CSV/Color-coded maps - 2024 Population Age 60-64.csv', dtype={'Census Tract': 'str'})\n",
    "df_60 = df_60.rename(columns={\n",
    "    'Census Tract': 'GEOID'\n",
    "})\n",
    "# df_60['GEOID'] = df_60['GEOID'].astype(float).map(\n",
    "#     lambda x: '{:.2f}'.format(x).replace('.', ''))\n",
    "\n",
    "df_senior_master = df_senior.merge(\n",
    "    df_55,\n",
    "    how='left',\n",
    "    on='GEOID'\n",
    ")\n",
    "\n",
    "df_senior_master = df_senior_master.merge(\n",
    "    df_60,\n",
    "    how='left',\n",
    "    on='GEOID'\n",
    ")\n",
    "\n",
    "df_senior_master['senior_total'] = df_senior_master['2024 Senior Population'] + \\\n",
    "    df_senior_master['2024 Population Age 55-59'] + \\\n",
    "    df_senior_master['2024 Population Age 60-64']\n",
    "df_senior_master = df_senior_master[[\n",
    "    'GEOID',\n",
    "    'senior_total'\n",
    "]]\n",
    "\n",
    "df_senior_master = df_senior_master.rename(\n",
    "    columns={'senior_total': '2024 Senior Population'})\n",
    "df_senior_master.to_csv(\n",
    "    'CSV/Color-coded maps - 2024 Senior Population_55.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_senior = pd.read_csv(\n",
    "    'CSV/Color-coded maps - 2024 Senior Population.csv', dtype={'GEOID': 'str'})\n",
    "df_senior['GEOID'] = df_senior['GEOID'].astype(float).map(\n",
    "    lambda x: '{:.2f}'.format(x))\n",
    "\n",
    "df_55 = pd.read_csv(\n",
    "    'CSV/Color-coded maps - 2024 Population Age 55-59.csv', dtype={'Census Tract': 'str'})\n",
    "df_55 = df_55.rename(columns={\n",
    "    'Census Tract': 'GEOID'\n",
    "})\n",
    "\n",
    "df_60 = pd.read_csv(\n",
    "    'CSV/Color-coded maps - 2024 Population Age 60-64.csv', dtype={'Census Tract': 'str'})\n",
    "df_60 = df_60.rename(columns={\n",
    "    'Census Tract': 'GEOID'\n",
    "})\n",
    "\n",
    "df_senior_master = df_senior.merge(\n",
    "    df_55,\n",
    "    how='left',\n",
    "    on='GEOID'\n",
    ")\n",
    "\n",
    "df_senior_master = df_senior_master.merge(\n",
    "    df_60,\n",
    "    how='left',\n",
    "    on='GEOID'\n",
    ")\n",
    "\n",
    "df_senior_master['senior_total'] = df_senior_master['2024 Senior Population'] + \\\n",
    "    df_senior_master['2024 Population Age 55-59'] + \\\n",
    "    df_senior_master['2024 Population Age 60-64']\n",
    "df_senior_master = df_senior_master[[\n",
    "    'GEOID',\n",
    "    'senior_total'\n",
    "]]\n",
    "\n",
    "df_senior_master = df_senior_master.rename(\n",
    "    columns={'senior_total': '2024 Senior Population'})\n",
    "df_senior_master.to_csv(\n",
    "    'CSV/Color-coded maps - 2024 Senior Population_55.csv', index=False)\n",
    "\n",
    "df_senior_master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParclLabs home sales data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded - Step 1 complete!\n",
      "14 metro county IDs acquired - Step 2 complete!\n",
      "Searching for single-family homes...\n",
      "Processing Parcl IDs |████████████████████████████████████████| 1/1 [100%] in 0.4s (2.34/s) \n",
      "Processing Parcl IDs |████████████████████████████████████████| 1/1 [100%] in 0.5s (1.99/s) \n",
      "Processing Parcl IDs |████████████████████████████████████████| 1/1 [100%] in 1.2s (0.80/s) \n",
      "Processing Parcl IDs |████████████████████████████████████████| 1/1 [100%] in 0.5s (1.99/s) \n",
      "Processing Parcl IDs |████████████████████████████████████████| 1/1 [100%] in 0.4s (2.72/s) \n",
      "Processing Parcl IDs |████████████████████████████████████████| 1/1 [100%] in 0.4s (2.36/s) \n",
      "Processing Parcl IDs |████████████████████████████████████████| 1/1 [100%] in 1.3s (0.76/s) \n",
      "Processing Parcl IDs |████████████████████████████████████████| 1/1 [100%] in 0.7s (1.44/s) \n",
      "Processing Parcl IDs |████████████████████████████████████████| 1/1 [100%] in 1.5s (0.65/s) \n",
      "Processing Parcl IDs |████████████████████████████████████████| 1/1 [100%] in 0.5s (2.10/s) \n",
      "Processing Parcl IDs |████████████████████████████████████████| 1/1 [100%] in 0.9s (1.12/s) \n",
      "Processing Parcl IDs |████████████████████████████████████████| 1/1 [100%] in 0.5s (1.98/s) \n",
      "Processing Parcl IDs |████████████████████████████████████████| 1/1 [100%] in 1.1s (0.90/s) \n",
      "Processing Parcl IDs |████████████████████████████████████████| 1/1 [100%] in 1.3s (0.76/s) \n",
      "Metro assessor file created - Step 3 complete!\n",
      "Searching for sales events since 2023-04-02...\n",
      "Processing Parcl Property IDs |████████████████████████████████████████| 36063/36063 [100%] in 1.1s (32629.27/s) \n",
      "Finished property events search - Step 4 complete!\n",
      "Spatially joined with tracts - Step 5 complete!\n",
      "Tract aggregation created & exported - Step 6 complete!\n",
      "Spatially joined with counties - Step 7 complete!\n",
      "County KPI CSV updated with new sales data - Step 8 complete! And script is done!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: load API key\n",
    "load_dotenv('../Unused/.env')\n",
    "api_key = os.getenv('PARCL_LABS_API_KEY')\n",
    "client = ParclLabsClient(\n",
    "    api_key=api_key\n",
    ")\n",
    "print('API key loaded - Step 1 complete!')\n",
    "\n",
    "# Step 2: create dictionary of county names & Parcl id values\n",
    "metro_county_ids = []\n",
    "for county in county_dict.keys():\n",
    "    initial_query = client.search.markets.retrieve(\n",
    "        geoid=county\n",
    "    )\n",
    "    county_id = initial_query['parcl_id'].values[0]\n",
    "    metro_county_ids.append(county_id)\n",
    "print(f'{len(metro_county_ids)} metro county IDs acquired - Step 2 complete!')\n",
    "\n",
    "# Step 3: create new assessor dataframe with all single-family homes\n",
    "print('Searching for single-family homes...')\n",
    "assessor_df = []\n",
    "for county_id in metro_county_ids:\n",
    "    homes = client.property.search.retrieve(\n",
    "        parcl_ids=[county_id],\n",
    "        property_type='SINGLE_FAMILY',\n",
    "        year_built_min=2020\n",
    "    )\n",
    "    homes = homes[[\n",
    "        'parcl_property_id',\n",
    "        'address',\n",
    "        'city',\n",
    "        'latitude',\n",
    "        'longitude',\n",
    "        'square_footage',\n",
    "        'year_built',\n",
    "    ]]\n",
    "    assessor_df.append(homes)\n",
    "\n",
    "assessor_combined = pd.concat(assessor_df, ignore_index=True)\n",
    "print('Metro assessor file created - Step 3 complete!')\n",
    "\n",
    "# Step 4: get sales events for the IDs in the assessor file ----------------------\n",
    "months_to_go_back = 18\n",
    "end_date, start_date = get_dates(months_to_go_back)\n",
    "\n",
    "parcl_property_ids = assessor_combined['parcl_property_id'].unique()\n",
    "\n",
    "print(f'Searching for sales events since {start_date}...')\n",
    "# run this through the events function\n",
    "sales = client.property.events.retrieve(\n",
    "    parcl_property_ids=parcl_property_ids,\n",
    "    event_type='SALE',\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    ")\n",
    "print('Finished property events search - Step 4 complete!')\n",
    "\n",
    "# remove zero-dollar transfers\n",
    "sales = sales[sales['price'] > 0]\n",
    "\n",
    "# remove non arms length\n",
    "sales = sales[sales['event_name'] == 'SOLD']\n",
    "\n",
    "# clean up the dataframe\n",
    "sales = sales[[\n",
    "    'parcl_property_id',\n",
    "    'price'\n",
    "]]\n",
    "\n",
    "# merge with assessor data to get property data\n",
    "sales_with_assessor = pd.merge(\n",
    "    sales,\n",
    "    assessor_combined,\n",
    "    how='left',\n",
    "    on='parcl_property_id'\n",
    ")\n",
    "\n",
    "# Step 5: spatial join with tracts & county layers\n",
    "sales_geo = gpd.GeoDataFrame(\n",
    "    sales_with_assessor,\n",
    "    geometry=gpd.points_from_xy(\n",
    "        sales_with_assessor['longitude'], sales_with_assessor['latitude']),\n",
    "    crs=\"EPSG:4269\"\n",
    ")\n",
    "\n",
    "# tracts first\n",
    "tracts = gpd.read_file('tracts_simp.gpkg')\n",
    "sales_tract = gpd.sjoin(sales_geo, tracts, predicate='within')\n",
    "print('Spatially joined with tracts - Step 5 complete!')\n",
    "tract_agg = sales_tract.groupby('GEOID').agg(\n",
    "    total_sales=('parcl_property_id', 'count'),\n",
    "    median_price=('price', 'median'),\n",
    "    median_SF=('square_footage', 'median')\n",
    ").reset_index()\n",
    "tract_agg.to_csv('Parcl_Recorder/tract_aggregation.csv', index=False)\n",
    "print('Tract aggregation created & exported - Step 6 complete!')\n",
    "\n",
    "# then counties\n",
    "counties = gpd.read_file('counties_simp.gpkg')\n",
    "sales_county = gpd.sjoin(sales_geo, counties, predicate='within')\n",
    "print('Spatially joined with counties - Step 7 complete!')\n",
    "county_agg = sales_county.groupby('county_stripped').agg(\n",
    "    total_sales=('parcl_property_id', 'count'),\n",
    "    median_price=('price', 'median'),\n",
    "    median_SF=('square_footage', 'median')\n",
    ").reset_index().rename(columns={'county_stripped': 'county'})\n",
    "new_total_sales = county_agg['total_sales'].to_list()\n",
    "new_median_price = county_agg['median_price'].to_list()\n",
    "new_median_SF = county_agg['median_SF'].to_list()\n",
    "countyKPI = pd.read_csv('County_CSV/countyKPI.csv')\n",
    "countyKPI['total_sales'] = new_total_sales\n",
    "countyKPI['median_price'] = new_median_price\n",
    "countyKPI['median_SF'] = new_median_SF\n",
    "\n",
    "countyKPI['total_sales'] = countyKPI['total_sales'].astype(\n",
    "    str).apply(lambda x: f\"{int(x):,}\")\n",
    "countyKPI['median_price'] = countyKPI['median_price'].astype(\n",
    "    str).apply(lambda x: f\"${float(x):,.0f}\")\n",
    "countyKPI['median_SF'] = countyKPI['median_SF'].astype(\n",
    "    str).apply(lambda x: f\"{float(x):,.0f}\")\n",
    "\n",
    "countyKPI.to_csv('County_CSV/countyKPI.csv', index=False)\n",
    "print('County KPI CSV updated with new sales data - Step 8 complete! And script is done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
